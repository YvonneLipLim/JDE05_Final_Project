{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <span style=\"color:darkblue; font-weight:bold;\">Generation SG Junior Data Engineer Programme</span><br><span style=\"color:darkblue; font-weight:regular;\">Final Project: Brazilian E-commerce Public Dataset by Olist</span><br>\n",
    "#### By Team C.H.Y.E.F - Chew How | Hui Shan | Pin Pin, Yvonne | Eunica | Yiping, Fred\n",
    "***\n",
    "<br>\n",
    "\n",
    "<a id=\"readme-top\"></a>\n",
    "<!-- TABLE OF CONTENTS -->\n",
    "<details>\n",
    "  <summary><h3><b>Table of Contents</b></h3></summary>\n",
    "  <ol>\n",
    "    <li>\n",
    "      <a href=\"#about-the-project\">About The Project</a>\n",
    "    </li>\n",
    "    <li>  \n",
    "      <a href=\"#project-objective\">Project Objectives</a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#project-deliverables\">Project Deliverables</a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#getting-started\">Getting Started</a>\n",
    "      <ul>\n",
    "        <li><a href=\"#prerequisites\">Prerequisites</a></li>\n",
    "        <li><a href=\"#installation\">Installation</a></li>\n",
    "      </ul>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#data-architecture\">Data Architecture</a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#data-sources\">Data Sources</a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#data-ingestion\">Data Ingestion</a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#data-Transformation\">Data Transformation</a>\n",
    "    </li>    \n",
    "    <li>\n",
    "      <a href=\"#data-warehousing\">Data Warehousing</a>\n",
    "    </li>\n",
    "    <li>\n",
    "      <a href=\"#dashboarding\">Dashboarding</a>\n",
    "    </li> \n",
    "     <li>\n",
    "      <a href=\"#Data Governance and Security\">Data Governance and Security</a>\n",
    "    </li>          \n",
    "  </ol>\n",
    "</details>\n",
    "<br><br>\n",
    "\n",
    "<!-- ABOUT THE PROJECT -->\n",
    "<details>\n",
    "  <summary><h3><b>About The Project</b></h3></summary>\n",
    "  <img src=\"https://raw.githubusercontent.com/YvonneLipLim/Images/main/Olist.png\" alt=\"Olist Store\" style=\"display: block; margin: 0 auto\" /><br><br>\n",
    "\n",
    "**Unlocking Growth in Brazil's Digital Marketplace: The Olist Story**<br>In the bustling heart of Brazil's e-commerce landscape, Olist Store stands as a testament to the country's digital transformation. As Brazil's leading marketplace department store, Olist has connected thousands of merchants with millions of customers, facilitating over 100,000 transactions between 2016 and 2018. Behind each order lies a story – a customer's journey, a merchant's reputation, and Olist's promise to deliver excellence.<br>\n",
    "\n",
    "**The Digital Goldmine**<br>Imagine a vast digital archive containing the pulse of Brazil's e-commerce: every order, every payment, every customer interaction meticulously recorded. This isn't just data – it's the collective experience of countless Brazilian shoppers, from the sunny beaches of Rio to the bustling streets of São Paulo. The dataset captures:\n",
    "- The rhythm of customer purchasing patterns\n",
    "- The flow of payments through Brazil's digital economy\n",
    "- The intricate dance of logistics across South America's largest country\n",
    "- The honest voices of customers sharing their experiences\n",
    "- The geographic tapestry of Brazil's diverse marketplace\n",
    "<br>\n",
    "\n",
    "**The Hidden Opportunity**<br>Yet beneath this mountain of data lies untapped potential. While Olist has successfully facilitated thousands of transactions, the true value isn't in what these numbers show – it's in what they hide. Every delayed delivery tells a story about optimization opportunities. Every customer review whispers insights about service improvement. Every abandoned cart holds clues to untapped revenue.<br>\n",
    "\n",
    "**The Challenge Before Us**<br>The task at hand is exciting and daunting: transforming this rich dataset into actionable intelligence that can revolutionize Olist's marketplace performance. We must:\n",
    "1. Decode patterns in customer behaviour that could unlock new market segments\n",
    "2. Uncover inefficiencies in fulfilment that, if addressed, could delight customers\n",
    "3. Identify the subtle factors that turn satisfied customers into loyal advocates\n",
    "4. Map the journey from browse to buy, understanding where customers hesitate and why<br>\n",
    "\n",
    "This isn't just about analyzing numbers – it's about understanding the stories of millions of Brazilians who shop online, the merchants who serve them, and the platform that brings them together. The insights we uncover could reshape the future of e-commerce in Latin America's largest economy.<br>\n",
    "\n",
    "The question isn't just \"What does the data tell us?\" but rather \"How can we use these insights to create a marketplace that serves Brazil better?\"<br>\n",
    "\n",
    "**Data Source:** [Kaggle - Brazilian E-commerce Public Dataset by Olist](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)<br>\n",
    "\n",
    "</details>\n",
    "<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n",
    "<br><br>\n",
    "\n",
    "<!-- PROJECT OBJECTIVES -->\n",
    "<details>\n",
    "  <summary><h3><b>Project Objectives</b></h3></summary>\n",
    "\n",
    "**The Vision: From Raw Data to Business Gold: A Digital Transformation Journey**<br>Imagine walking into a room filled with scattered pieces of a puzzle. Each piece represents a fragment of your business's story – customer interactions, operational metrics, market performance, and countless other data points. Right now, these pieces lie dormant, their true value locked away. But we're about to change that.<br>\n",
    "\n",
    "**The Challenge**<br>In today's data-driven world, having information isn't enough. The real power lies in how we collect, connect, and interpret it. Our mission is to build a robust digital pipeline that will transform raw data into actionable insights, enabling leaders to make decisions with confidence and precision.<br>\n",
    "\n",
    "**The Journey**<br>\n",
    "**Phase 1: Building the Foundation**<br>Picture constructing a digital highway where data flows seamlessly from various sources into a centralized, intelligent system. We're not just storing data – we're creating a living, breathing ecosystem in our PostgreSQL/SQL Server database. Every table, every relationship, every data point is carefully architected to tell a story.<br>\n",
    "\n",
    "**Phase 2: Quality and Discovery**<br>Like a master jeweler examining precious stones, we'll scrutinize every piece of data:\n",
    "- Uncovering hidden patterns in customer behavior\n",
    "- Identifying anomalies that could signal opportunities or risks\n",
    "- Cleaning and enriching data to ensure it's trustworthy and meaningful\n",
    "- Connecting seemingly unrelated information to reveal deeper insights\n",
    "<br>\n",
    "\n",
    "**Phase 3: Bringing Data to Life**<br>This is where numbers transform into narratives. Through an intuitive Power BI dashboard:\n",
    "- Complex data patterns become clear visual stories\n",
    "- Real-time insights emerge at the click of a button\n",
    "- Business leaders can explore data landscapes with ease\n",
    "- Teams across departments gain a shared view of truth\n",
    "<br>\n",
    "\n",
    "**Phase 4: Ensuring Excellence**<br>Trust is earned through rigorous validation. Every insight, every calculation, every visualization will be meticulously tested. We'll compare dashboard figures against database queries, ensuring that what you see isn't just beautiful – it's absolutely accurate.<br>\n",
    "\n",
    "**Phase 5: Unleashing Strategic Power**<br>The culmination of our journey isn't just a technical achievement – it's a business transformation. Armed with deep insights, we'll:\n",
    "- Identify untapped market opportunities\n",
    "- Optimize operational efficiency\n",
    "- Enhance customer experiences\n",
    "- Drive data-informed strategic decisions\n",
    "<br>\n",
    "\n",
    "**The Impact**<br>When complete, this isn't just a data project – it's a business revolution. Decision-makers will move from gut feelings to data-driven certainty. Teams will shift from reactive to proactive. And your organization will gain a competitive edge through deeper understanding and faster response to market dynamics.<br>\n",
    "\n",
    "**Looking Ahead**<br>This transformation journey marks the beginning of a new era for your business – one where every decision is informed by insights, every strategy is backed by data, and every team is empowered with knowledge. The future isn't just about having data; it's about mastering it.\n",
    "</details>\n",
    "<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n",
    "<br><br>\n",
    "\n",
    "<!-- PROJECT DELIVERABLES -->\n",
    "<details>\n",
    "  <summary><h3><b>Project Deliverables</b></h3></summary>\n",
    "  <li>Information about the architecture, dataset, data relationships, database schema</li>\n",
    "  <li>Approach, challenges, findings, recommendations</li>\n",
    "  <li>Power BI source file or the published Power BI dashboard URL</li>\n",
    "  <li>Source code</li>\n",
    "</details>\n",
    "<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n",
    "<br>\n",
    "\n",
    "<!-- GETTING STARTED -->\n",
    "<a id=\"getting_started-top\"></a>\n",
    "<details>\n",
    "  <summary><h3><b>Getting Started</b></h3></summary>\n",
    "\n",
    "### <span style=\"color:darkblue; font-weight:bold;\">Prerequisites</span>\n",
    "\n",
    "**Azure Managed Services**\n",
    "  ```sh\n",
    "  ├── 01. Azure Subscription\n",
    "  ├── 02. Kaggle API Credentials \n",
    "  ├── 03. Azure Command-Line Interface (CLI)\n",
    "  ├── 04. Azure Storage / Containers\n",
    "  ├── 05. Azure Function\n",
    "  ├── 06. Azure Data Factory\n",
    "  ├── 07. Azure Databricks\n",
    "  ├── 08. Azure SQL Database\n",
    "  ├── 09. Azure Synapse Analytics\n",
    "  ├── 10. Power BI\n",
    "  ```\n",
    "<br>\n",
    "\n",
    "**1. Azure Subscription**<br>To access to Azure, you'ii need an Azure subscription. If you don't already have a subscription, create a [free account](https://azure.microsoft.com/en-us/pricing/purchase-options/azure-account?icid=azurefreeaccount&WT.mc_id=A261C142F) before you begin.<br>\n",
    "\n",
    "**2. Kaggle API Credentials**<br>A **Kaggle API token** allows you to access and interact with Kaggle's features programmatically throught their public API, enablinh you to perform actions like downloading datasets, submitting competition entries, or managing notebooks directly from your code.<br>\n",
    "\n",
    "> **How to obtain a Kaggle API Key?**<br>\n",
    "> Go to the `Account` tab of your user profile and select `Create New Token`. This will trigger the download of the `kaggle.json`, a file containing your API credentials.\n",
    "> \n",
    "\n",
    "**3. Azure Command-Line Interface (CLI)**<br>The [Azure Command-Line Interface (CLI)](https://learn.microsoft.com/en-us/cli/azure/get-started-with-azure-cli) is a tool that allows users to interact with Azure services and resources using a terminal. It's a cross-platform tool that can be used to create, manage, and deploy resources like databases, virtual machines, and storage accounts.<br>\n",
    "\n",
    "**4. Azure Storage / Container(s)**<br>Set up a storage account to store downloaded datasets.<br>An Azure storage account is a container that bands a set of Azure Storage services together, click on [Microsoft Learn](https://learn.microsoft.com/en-us/azure/storage/common/storage-account-create?tabs=azure-portal).<br>After setting up a storage account, you will need to create a [storage container](https://learn.microsoft.com/en-us/azure/storage/blobs/blob-containers-portal#create-a-container) for storing the datasets.<br>\n",
    "\n",
    "**5. Azure Function**<br>Set up an [Azure Function App](https://learn.microsoft.com/en-us/azure/azure-functions/functions-create-function-app-portal?pivots=programming-language-csharp#create-a-function-app) to create a serverless environment for running custom code on-demand for data processing tasks. This eliminates infrastructure management and is ideal for small, event-driven operations, allowing for flexible scaling and cost-effective data manipulation.<br>\n",
    "\n",
    "**6. Azure Data Factory**<br>Set up an [Azure Data Factory (ADF)](https://learn.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory) account to manage data pipelines that extract, transform, and load data from diverse sources including on-premises, cloud, SaaS systems. This user-friendly platform simplifies data integration at scale without complex infrastructure, offering extensive connectors and serverless execution while requiring minimal code required.<br>\n",
    "\n",
    "**7. Azure Databricks**<br>Set up [Azure Databricks](https://learn.microsoft.com/en-us/azure/databricks/getting-started/) workspace provides a unified platform in Azure for data processing, transformation, and analytics. It streamlines data pipeline development with features like Delta Lake and Apache Spark, enabling efficient handling of large-scale data operations.<br>\n",
    "\n",
    "**8. Azure SQL Database**<br>Set up [Azure SQL Database](https://learn.microsoft.com/en-us/azure/azure-sql/database/single-database-create-quickstart?view=azuresql&tabs=azure-portal) which is a managed cloud database (PaaS) cloud-based Microsoft SQL Servers, provided as parts of Microsoft Azure services. The service handles database management functions for cloud based Microsoft SQL Servers including upgrading, patching, backups, and monitoring without user involvement.<br>\n",
    "\n",
    "**9. Azure Synapse Analytics**<br>Set up [Azure Synapse Analytics](https://learn.microsoft.com/en-us/azure/synapse-analytics/quickstart-create-workspace), a unified cloud-based analytics platform from Microsoft that combines data warehousing, big data processing, and data integration capabilities, allowing users to query and analyze large datasets from various sources using a single interface, including both relational and non-relational data.<br>\n",
    "\n",
    "**10. Power BI Desktop**<br>Set up  [Power BI Desktop](https://learn.microsoft.com/en-us/power-bi/fundamentals/desktop-getting-started) which is a collection of software services, apps, and connectors that work together to turn your unrelated sources of data into coherent, visually immersive, and interactive insights. Power BI lets you easily connect to your data sources, visualize and discover what's important, and share that with anyone or everyone you want.<br>\n",
    "\n",
    "**Azure Account Structure**<br>Organizing resources effectively in Microsoft Azure is essential for managing a cloud environment. Azure uses a structured hierarchy that helps businesses and individuals manage resources more efficiently, control costs, and ensure proper governance. In this guide, we break down the Azure resource hierarchy in simple terms to give you a clear understanding of how to use it.\n",
    "  ```sh\n",
    "  ├── 01. Account\n",
    "  ├── 02. Subscription \n",
    "  ├── 03. Resource Group\n",
    "  ├── 04. Resources\n",
    "  ```\n",
    "<br>\n",
    "<img src=\"https://raw.githubusercontent.com/YvonneLipLim/Images/main/OLIST_Azure_Account_Structure.png\" alt=\"Olist Store\" style=\"display: block; margin: 0 auto\" /><br><br>\n",
    "\n",
    "### <span style=\"color:darkblue; font-weight:bold;\">Installation</span>\n",
    "**1. Kaggle CLI**<br>The **Kaggle CLI** tool provive easy ways to interact with Datasets on Kaggle. These commands available can make searching for and downloading Kaggle Datasets a seamless part of data workflow.\n",
    "1. Run Installation Command on a new Terminal:\n",
    "    ```sh\n",
    "    pip install kaggle\n",
    "    ```\n",
    "2. After installation is completed, you can verify it by running `kaggle --version` to check version.\n",
    "3. Move the `kaggle.json` file to the `.kaggle directory` by running:\n",
    "    ```sh\n",
    "    mkdir ~/.kaggle\n",
    "    mv ~/Downloads/kaggle.json ~/.kaggle/\n",
    "    ```\n",
    "4. List the current active competition list to confirm if Kaggle CLI is functioning well:\n",
    "    ```sh\n",
    "    kaggle competitions list\n",
    "    ```\n",
    "<br>\n",
    "\n",
    "**2. Azure CLI**<br>The **Azure CLI** can be installed on Windows, macOS and Linux distributions: \n",
    "* On Linus and macOS, most commonly use package managers like Homebrew or apt-get to install the Azure CLI. \n",
    "* While on windows, you can install the Azure CLI using a Microsoft Installer (MSI) Package<br><br>\n",
    "\n",
    "How to install Azure Commmand-Line Interface (CLI)?\n",
    "1. Run Installation Command on a new Terminal:\n",
    "    ```sh\n",
    "    macOS with Homebrew: brew install azure-cli\n",
    "    Linux (Ubuntu, Debian): sudo apt-get install azure-cli\n",
    "    Windows (PowerShell): iex ((New-Object System.Net.WebClient).DownloadString('https://raw.githubusercontent.com/Azure/azure-cli/latest/azure/install.ps1'))\n",
    "    ```\n",
    "2. After installation is completed, you can verify it by running `az --version` to check version.\n",
    "3. Login to Azure by running `az login` in your terminal to sign in with your Azure account.<br>\n",
    "For more information on Azure CLI services, click on [Microsoft Learn](https://learn.microsoft.com/en-us/cli/azure/install-azure-cli).<br><br>\n",
    "\n",
    "**3. Azure MyFunction App**<br>The [Azure Functions Core Tools](https://learn.microsoft.com/en-us/azure/azure-functions/functions-run-local?tabs=macos%2Cisolated-process%2Cnode-v4%2Cpython-v2%2Chttp-trigger%2Ccontainer-apps&pivots=programming-language-csharp) can be installed on Windows, macOS and Linux distributions.<br>\n",
    "\n",
    "How to install Azure Commmand-Line Interface (CLI)?\n",
    "1. Run Installation Command on a new Terminal for macOS: \n",
    "    ```sh\n",
    "    brew tap azure/functions\n",
    "    brew install azure-functions-core-tools@4\n",
    "    # if upgrading on a machine that has 2.x or 3.x installed:\n",
    "    brew link --overwrite azure-functions-core-tools@4\n",
    "    ```\n",
    "2. Create your local project in a Python Programming model, run the following command:\n",
    "   ```sh\n",
    "   func init MyFunctionApp --python\n",
    "   ```\n",
    "3. A new folder can be found in your local drive:\n",
    "   ```sh\n",
    "   /Users/User-Name/MyFunctionApp/\n",
    "    ├── function_app.py\n",
    "    ├── host.json\n",
    "    ├── local.settings.json\n",
    "    ├── requirements.txt\n",
    "    ├── .vscode/extensions.json\n",
    "   ```\n",
    "4. Update `requirements.txt` file by adding the following dependencies: \n",
    "   ```sh\n",
    "   azure-functions\n",
    "   azure-storage-blob\n",
    "   kaggle\n",
    "   ```\n",
    "   Then run this command:\n",
    "   ```sh\n",
    "   pip install -r requirements.txt\n",
    "   ```   \n",
    "   Alternatively, you can run this command:\n",
    "   ```sh\n",
    "   pip install azure-functions azure-storage-blob kaggle\n",
    "   ```\n",
    "5. Add your storage connection string to the `local.settings.json` file. You can find this in Azure portal under the storage account's \"Access keys\" section:\n",
    "   ```sh\n",
    "   {\n",
    "     \"IsEncrypted\": false,\n",
    "     \"Values\": {\n",
    "      \"AzureWebJobsStorage\": \"UseDevelopmentStorage=true\",\n",
    "      \"FUNCTIONS_WORKER_RUNTIME\": \"python\",\n",
    "      \"AZURE_STORAGE_CONNECTION_STRING\": \"your_actual_connection_string_here\"\n",
    "     }\n",
    "   }\n",
    "   ```\n",
    "6. Install Azure Storage Emulator as we are using \"UseDevelopmentStorage=true\"\n",
    "   ```sh\n",
    "   npm install -g azurite\n",
    "   ```\n",
    "\n",
    "</details>\n",
    "<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n",
    "<br><br>\n",
    "\n",
    "<!-- DATA ARCHITECTURE -->\n",
    "<details>\n",
    "  <summary><h3><b>Data Architecture</b></h3></summary>\n",
    "  <p>\n",
    "  Building a data pipeline with Azure as the primary environment and AWS as a secondary environment offers several strategic business adventages. This approach leverages the strengths of both cloud platforms while providing redundancy and flexiblity. Here's an explanation of the business rationale behind this decision:</p><br>\n",
    "\n",
    "**Azure (primary environment)**<br>It allows to efficiently store, process, and analyze large volumes of e-commerce data from Olist platform, enabling them to extract valuable insights for better decision-making, optimize operations, predict trends, and ultimately improve customer experience by leveraging the scalability, flexibility, and powerful analytics tools offered by Azure services like Azure Data Lake, Azure Data Factory, and Azure Synapse Analytics, particularly when dealing with the vast and diverse data sets generated by an online marketplace.<br>\n",
    "\n",
    "**Key reasons**<br>\n",
    "- **Large Data Volumes:**<br>Olist generates a significant amount of transactional data from sellers, buyers, products, and logistics, which can be easily stored and processed in Azure Data Lake due to its large storage capacity and distributed processing capabilities.<br>\n",
    "- **Data Variety:**<br>With different data types like customer demographics, product details, order information, and shipping logistics, Azure allows for flexible data ingestion and analysis of both structured and unstructured data. \n",
    "- **Scalability:**<br>As Olist grows, the Azure platform can seamlessly scale up to accommodate increased data volume and processing needs without requiring significant infrastructure management. \n",
    "- **Advanced Analytics:**<br>With Azure Synapse Analytics, Olist can perform complex data analysis, build predictive models, and generate actionable insights using machine learning techniques. \n",
    "- **Cost-Effectiveness:**<br>Compared to managing on-premise data infrastructure, Azure offers a pay-as-you-go model, allowing Olist to optimize costs based on their data processing needs.\n",
    "<br><br>\n",
    "<img src=\"https://raw.githubusercontent.com/YvonneLipLim/Images/main/OLIST_Azure_Architecturev2.0.png\" alt=\"Olist Azure Architecture\" style=\"display: block; margin: 0 auto\" /><br><br>\n",
    "\n",
    "**AWS (secondary environment)**<br>Incorporating AWS as a secondary environment provides additional benefits and serves as a strategic backup solution.<br>\n",
    "\n",
    "**Key reasons**<br>\n",
    "- **Disaster Recovery and Business Continuity:**<br>By implementing AWS Data pipeline as a secondary environment, Olist create a robust disaster recovery solution. In case of any issues with the promary Azure environment, data processing can be quickly shifted to AWS, ensuring business continuity.\n",
    "- **Multi-cloud Strateg:y**<br>Utilizing both Azure and AWS aligns with a multi-cloud strategy, reducing vendor lock-in and providing flexbility in chossing services that best fit specifc needs.\n",
    "- **Geographical Redundancy:**<br>AWS's global infrastructure can complement Azure's offering additional geographical redudancy for data storage and processing. This is particulary beneficials for Olist with a global presence or subject to data residency requirements.\n",
    "- **Cost Optimization:**<br>AWS Data Pipeline's pay-as-you-go pricing model allows businesses to optimize costs by using it as a secondary or backup solution. This approach ensures that company only pay for the resources they use when needed.\n",
    "<br><br>\n",
    "<img src=\"https://raw.githubusercontent.com/YvonneLipLim/Images/main/OLIST_AWS_Architecturev1.0.png\" alt=\"Olist AWS Architecture\" style=\"display: block; margin: 0 auto\" /><br><br>\n",
    "\n",
    "**Business Rational**<br>\n",
    "1. **Risk Mitigation:**<br>By diversifying across two major cloud providers, Olist reduces the risk of service disruptions and ensure continous data processing capabilities.\n",
    "2. **Flexbility and Innovation:**<br>Access to both Azure and AWS ecosystems allows organizations to leverage the best features and services from each platform, fostering innovation in data management and analytics.\n",
    "3. **Compliance and Data Governance:**<br>The ability to distribute data across multiple cloud environments can help in meeting various regulatory requirements and data governance policies.\n",
    "4. **Competitive Advantage:**<br>A dual-cloud strategy for data pipelines positions the company as technologically advanced and adaptable, potentially giving it an edge over competitors who rely on a single cloud provider.\n",
    "5. **Future-proofing:**<br>As cloud technologites evolve, having expertise and infrastructure in both Azure and AWS ensures that the organization can quickly adapt to new developments and maintain a cutting-edge data management strategy.<br>\n",
    "By implementing this dual-cloud approach for data pipelines, Olist can create a robust, flexible and efficient data management infrastructure that supports their current needs while preparing them for future challenges and opportunities.<br>\n",
    "\n",
    "</details>\n",
    "<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n",
    "<br><br>\n",
    "\n",
    "<!-- DATA SOURCES -->\n",
    "<details>\n",
    "  <summary><h3><b>Data Sources</b></h3></summary>\n",
    "\n",
    "**Datasets**<br>Olist stores have the following datasets:\n",
    "1. Customers information\n",
    "   ```sh\n",
    "   olist_customers_dataset.csv (5 columns):\n",
    "   customer_id: unique identifier for each customer\n",
    "   customer_unique_id: unique identifier for each customer (anonymized)\n",
    "   customer_zip_code_prefix: zip code prefix of the customer’s address\n",
    "   customer_city: city where the customer is located\n",
    "   customer_state: state where the customer is located\n",
    "   ```\n",
    "2. Geolocation\n",
    "   ```sh\n",
    "   olist_geolocation_dataset.csv (5 columns)\n",
    "   geolocation_zip_code_prefix: zip code prefix for the location\n",
    "   geolocation_lat: latitude of the location\n",
    "   geolocation_lng: longitude of the location\n",
    "   geolocation_city: city of the location\n",
    "   geolocation_state: state of the location\n",
    "   ```\n",
    "3. Order items\n",
    "   ```sh\n",
    "   olist_orders_dataset.csv (7 columns)\n",
    "   order_id: unique identifier for each order\n",
    "   order_item_id: unique identifier for each item within an order\n",
    "   product_id: unique identifier for the product being ordered\n",
    "   seller_id: unique identifier for the seller who listed the product\n",
    "   shipping_limit_date: date and time when the seller has to ship the product\n",
    "   price: price of the product\n",
    "   freight_value: shipping fee for the product\n",
    "   ```\n",
    "4. Order payments\n",
    "   ```sh\n",
    "   olist_order_payments_dataset.csv (5 columns)\n",
    "   order_id: unique identifier for the order\n",
    "   payment_sequential: index number for each payment made for an order\n",
    "   payment_type: type of payment used for the order (e.g. credit card, debit card, voucher)\n",
    "   payment_installments: number of installments in which the payment was made\n",
    "   payment_value: value of the payment made\n",
    "   ```\n",
    "5. Order reviews\n",
    "   ```sh\n",
    "   olist_order_reviews_dataset.csv (7 columns)\n",
    "   review_id: unique identifier for each review\n",
    "   order_id: unique identifier for the order that the review is associated with\n",
    "   review_score: numerical score (1-5) given by the customer for the product\n",
    "   review_comment_title: title of the review comment\n",
    "   review_comment_message: text of the review comment\n",
    "   review_creation_date: date and time when the review was created\n",
    "   review_answer_timestamp: date and time when the seller responded to the review (if\n",
    "   applicable)\n",
    "   ```\n",
    "6. Orders \n",
    "   ```sh\n",
    "   olist_orders_dataset.csv (8 columns)\n",
    "   order_id: unique identifier of the order\n",
    "   customer_id: unique identifier for the customer who placed the order\n",
    "   order_status: current status of the order (e.g. delivered, shipped, canceled)\n",
    "   order_purchase-timestamp: date and time when the order was placed\n",
    "   order_approved_at: date and time when the payment for the order was approved\n",
    "   order_delivered_carrier_data: date and time when the order was handed over to the carrier\n",
    "   order_delivered_customer_date: date and time when the order was delivered to the customer\n",
    "   order_estimated_delivery_date: estimated date when the order is expected to be delivered\n",
    "   ```\n",
    "7. Products\n",
    "   ```sh\n",
    "   olist_products_dataset.csv (9 columns)\n",
    "   product_id: unique identifier for each product\n",
    "   product_category_name: name of the category that the product belongs to\n",
    "   product_name_lenght: number of characters in the product name\n",
    "   product_description_lenght: number of characters in the product description\n",
    "   product_photos_qty: number of photos for the product\n",
    "   product_weight_g: weight of the product in grams\n",
    "   product_length_cm: length of the product in centimeters\n",
    "   product_height_cm: height of the product in centimeters\n",
    "   product_width_cm: width of the product in centimeters\n",
    "   ```\n",
    "8. Sellers information \n",
    "   ```sh\n",
    "   olist_sellers_dataset.csv (4 columns)\n",
    "   seller_id: unique identifier for each seller\n",
    "   seller_zip_code_prefix: zip code prefix for the seller's location\n",
    "   seller_city: city where the seller is located\n",
    "   seller_state: state where the seller is located\n",
    "   ```\n",
    "9. Product category name translation\n",
    "   ```sh\n",
    "   product_category_name_translation.csv (2 columns)\n",
    "   product_category_name: name of the product category in Portuguese\n",
    "   product_category_name_english: name of the product category in English\n",
    "   ```\n",
    "<br>\n",
    "The data pipeline starts by ingesting data from the on-premises Olist store which may include data from various sources such as sales transactions, customer information, product details, and order history.<br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/YvonneLipLim/Images/main/OLIST_Data_Schema.png\" alt=\"Olist Data Schema\" style=\"display: block; margin: 0 auto\" /><br>\n",
    "\n",
    "**Olist ERD Diagram**<br>An entity-relationship diagram (ERD) of the entities within the OLIST system or applications and the relationships between them.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/YvonneLipLim/Images/main/OLIST_Entity_Relationship.png\" alt=\"Olist ERD Diagram\" style=\"display: block; margin: 0 auto\" /><br><br>\n",
    "\n",
    "**Database Setup and Creation**<br>Reviewing datasets and setting up database tables are critical steps in ensuring data integrity, usability, and efficiency. Here's why they are important:\n",
    "1. **Data Accuracy and Quality:**<br>\n",
    "   - It helps identify and address missing, duplicate, or inconsistent data, ensuring the information is reliable and accurate.\n",
    "   - Proper table setup enforces data validation rules, reducing errors during data entry or processing.\n",
    "2. **Optimal Data Organization:**<br>\n",
    "   - Structuring database tables using normalization minimizes redundancy and organizes data logically, making it easier to retrieve and manage.\n",
    "   - A well-designed schema improves data readability and supports efficient relationships between datasets.\n",
    "3. **Performance Optimization:**<br>\n",
    "   - It allows for identifying and removing unnecessary data, improving query performance.\n",
    "   - Proper indexing and table partitioning during setup enhance the speed of data retrieval and processing.\n",
    "4. **Compliance and Security:**<br>\n",
    "   - Ensuring sensitive data is correctly stored and protected during the setup process helps meet regulatory and privacy standards (e.g., GDPR, HIPAA).\n",
    "   - Ensures no unauthorized or non-compliant information is stored.\n",
    "5. **Scalability and Future-Proofing:**<br>\n",
    "   - A well-reviewed and structured database is easier to scale as data volume increases.\n",
    "   - Future enhancements or integration with other systems become simpler when the foundational setup is robust.\n",
    "6. **Supporting Analytics and Reporting:**<br>\n",
    "   - Clean, structured datasets and properly designed tables are essential for accurate reporting and analytics.\n",
    "   - Ensures the inclusion of necessary fields and metrics for meaningful insights.\n",
    "7. **Collaboration and Cross-Team Understanding:**<br>\n",
    "   - A clear and logical database setup with well-documented datasets promotes better collaboration among teams (e.g., developers, analysts, and end-users).<br><br>\n",
    "  ```sh\n",
    "  -- OLIST: Brazilian E-Commerce Data Structure and Relationships\n",
    "\n",
    "  -- -------------------------------------------------\n",
    "  -- Table 1: Structure for customers\n",
    "  -- -------------------------------------------------\n",
    "  DROP TABLE IF EXISTS customers;\n",
    "  CREATE TABLE customers (\n",
    "\tcustomer_id VARCHAR(32) NOT NULL,\n",
    "\tcustomer_unique_id VARCHAR(32) NOT NULL,\n",
    "\tcustomer_zip_code_prefix INT NOT NULL,\n",
    "\tcustomer_city VARCHAR(255) NOT NULL,\n",
    "\tcustomer_state CHAR(2) NOT NULL,\n",
    "\tPRIMARY KEY (customer_id)\n",
    "  );\n",
    "\n",
    "  -- -------------------------------------------------\n",
    "  -- Table 2: Structure for geolocation\n",
    "  -- -------------------------------------------------\n",
    "  DROP TABLE IF EXISTS geolocation;\n",
    "  CREATE TABLE geolocation (\n",
    "\tgeolocation_zip_code_prefix INT NOT NULL,\n",
    "\tgeolocation_lat DECIMAL(12,8) NOT NULL,\n",
    "\tgeolocation_lng DECIMAL(12,8) NOT NULL,\n",
    "\tgeolocation_city VARCHAR(255) NOT NULL,\n",
    "\tgeolocation_state CHAR(2) NOT NULL\n",
    "  );\n",
    "\n",
    "  -- -------------------------------------------------\n",
    "  -- Table 3: Structure for products\n",
    "  -- -------------------------------------------------\n",
    "  DROP TABLE IF EXISTS products;\n",
    "  CREATE TABLE products (\n",
    "    product_id VARCHAR(32) NOT NULL,\n",
    "    product_category_name TEXT,\n",
    "    product_name_length INT,\n",
    "    product_description_length INT,\n",
    "    product_photos_qty INT,\n",
    "    product_weight_g INT,\n",
    "    product_length_cm INT,\n",
    "    product_height_cm INT,\n",
    "    product_width_cm INT,\n",
    "    PRIMARY KEY (product_id),\n",
    "\t  FOREIGN KEY (product_category_name) REFERENCES product_category_name_translation(product_category_name),\n",
    "    CONSTRAINT positive_measurements CHECK (\n",
    "        (product_weight_g IS NULL OR product_weight_g >= 0) AND  \n",
    "        (product_length_cm IS NULL OR product_length_cm > 0) AND\n",
    "        (product_height_cm IS NULL OR product_height_cm > 0) AND\n",
    "        (product_width_cm IS NULL OR product_width_cm > 0)\n",
    "    )\n",
    "  );\n",
    "\n",
    "  -- -------------------------------------------------\n",
    "  -- Table 4: Structure for sellers\n",
    "  -- -------------------------------------------------\n",
    "  DROP TABLE IF EXISTS sellers;\n",
    "  CREATE TABLE sellers (\n",
    "    seller_id VARCHAR(32) NOT NULL,\n",
    "    seller_zip_code_prefix INT NOT NULL,\n",
    "    seller_city VARCHAR(255) NOT NULL,\n",
    "    seller_state CHAR(2) NOT NULL,\n",
    "    PRIMARY KEY (seller_id)\n",
    "  );\n",
    "\n",
    "  -- -------------------------------------------------\n",
    "  -- Table 5: Structure for orders\n",
    "  -- -------------------------------------------------\n",
    "  DROP TABLE IF EXISTS orders;\n",
    "  CREATE TABLE orders (\n",
    "    order_id VARCHAR(32) NOT NULL,\n",
    "    customer_id VARCHAR(32) NOT NULL,\n",
    "    order_status VARCHAR(12) NOT NULL,\n",
    "    order_purchase_timestamp TIMESTAMP NOT NULL,\n",
    "    order_approved_at TIMESTAMP,\n",
    "    order_delivered_carrier_date TIMESTAMP,\n",
    "    order_delivered_customer_date TIMESTAMP,\n",
    "    order_estimated_delivery_date TIMESTAMP NOT NULL,\n",
    "    PRIMARY KEY (order_id),\n",
    "    FOREIGN KEY (customer_id) REFERENCES customers(customer_id),\n",
    "    CONSTRAINT valid_order_status CHECK (order_status IN ('approved', 'canceled', 'created', 'delivered', 'invoiced', 'processing', 'shipped', 'unavailable'))\n",
    "  );\n",
    "\n",
    "  -- -------------------------------------------------\n",
    "  -- Table 6: Structure for order items\n",
    "  -- -------------------------------------------------\n",
    "  DROP TABLE IF EXISTS order_items;\n",
    "  CREATE TABLE order_items (\n",
    "    order_id VARCHAR(32) NOT NULL,\n",
    "    order_item_id INT NOT NULL,\n",
    "    product_id VARCHAR(32) NOT NULL,\n",
    "    seller_id VARCHAR(32) NOT NULL,\n",
    "    shipping_limit_date TIMESTAMP NOT NULL,\n",
    "    price DECIMAL(12, 2) NOT NULL CHECK (price > 0),\n",
    "    freight_value DECIMAL(12, 2) NOT NULL CHECK (freight_value >= 0),\n",
    "    PRIMARY KEY (order_id, order_item_id),\n",
    "    FOREIGN KEY (product_id) REFERENCES products(product_id),\n",
    "    FOREIGN KEY (seller_id) REFERENCES sellers(seller_id),\n",
    "    FOREIGN KEY (order_id) REFERENCES orders(order_id)\n",
    "  );\n",
    "\n",
    "  -- -------------------------------------------------\n",
    "  -- Table 7: Structure for order payments\n",
    "  -- -------------------------------------------------\n",
    "  DROP TABLE IF EXISTS order_payments;\n",
    "  CREATE TABLE order_payments (\n",
    "    order_id VARCHAR(32) NOT NULL,\n",
    "    payment_sequential INT NOT NULL,\n",
    "    payment_type VARCHAR(20) NOT NULL,\n",
    "    payment_installments INT NOT NULL,\n",
    "    payment_value DECIMAL(12, 2) NOT NULL,\n",
    "    PRIMARY KEY (order_id, payment_sequential),\n",
    "    FOREIGN KEY (order_id) REFERENCES orders(order_id),\n",
    "    CONSTRAINT valid_payment_type CHECK (payment_type IN ('credit_card', 'boleto', 'voucher', 'debit_card', 'not_defined'))\n",
    "  );\n",
    "\n",
    "  -- -------------------------------------------------\n",
    "  -- Table 8: Structure for order reviews\n",
    "  -- -------------------------------------------------\n",
    "  DROP TABLE IF EXISTS order_reviews;\n",
    "  CREATE TABLE order_reviews (\n",
    "    review_id VARCHAR(32) NOT NULL,\n",
    "    order_id VARCHAR(32) NOT NULL,\n",
    "    review_score INT NOT NULL CHECK (review_score BETWEEN 1 AND 5),\n",
    "    review_comment_title TEXT,\n",
    "    review_comment_message TEXT,\n",
    "    review_creation_date TIMESTAMP NOT NULL,\n",
    "    review_answer_timestamp TIMESTAMP,  \n",
    "    PRIMARY KEY (review_id, order_id),  \n",
    "    FOREIGN KEY (order_id) REFERENCES orders(order_id)\n",
    "  );\n",
    "\n",
    "  -- -------------------------------------------------\n",
    "  -- Table 9: Structure for product catergory name translation\n",
    "  -- -------------------------------------------------\n",
    "  DROP TABLE IF EXISTS product_category_name_translation;\n",
    "  CREATE TABLE product_category_name_translation (\n",
    "    product_category_name TEXT NOT NULL,\n",
    "    product_category_name_english TEXT NOT NULL,\n",
    "    PRIMARY KEY (product_category_name)\n",
    "  );\n",
    "  ```\n",
    "\n",
    "</details>\n",
    "<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n",
    "<br><br>\n",
    "\n",
    "<!-- DATA INGESTION -->\n",
    "<details>\n",
    "  <summary><h3><b>Data Ingestion</b></h3></summary>\n",
    "\n",
    "There are 3 methods of data ingestion:\n",
    "1. Extracting datasets using Kaggle CLI or a Python script and upload to the Azure storage with Azure CLI.\n",
    "2. Write a Python Azure Function script to automate the data ingestion on a scheduled or manually basis.\n",
    "3. Extracting datasets from HTTP.\n",
    "<br>\n",
    "\n",
    "**Method 1**<br>Extracting Olist Datasets with Kaggle CLI**\n",
    "1. Download the Olist datasets by running this command on a new Terminal:\n",
    "    ```sh\n",
    "    kaggle datasets download -d \"olistbr/brazilian-ecommerce\"\n",
    "    ```\n",
    "2. Extract the Downloaded ZIP file:\n",
    "    ```sh\n",
    "    unzip brazilian-ecommerce.zip\n",
    "    ```\n",
    "3. Check the CSV file after unzipping:\n",
    "    ```sh\n",
    "    ls -l\n",
    "    ```\n",
    "<img src=\"https://raw.githubusercontent.com/YvonneLipLim/Images/main/Kaggle_Datasets_Download.png\" alt=\"Download Kaggle Datasets\" style=\"display: block; margin: 0 auto\" /><br><br>\n",
    "\n",
    "1. To search for datasets, run the following command for a complete list:\n",
    "    ```sh\n",
    "    kaggle datasets list -s <keyword>\n",
    "    ```\n",
    "\n",
    "2. If you want to download a particular file from the dataset, use this command:\n",
    "    ```sh\n",
    "    kaggle datasets download -d <owner>/<dataset-name> -f <file-name>\n",
    "    ```\n",
    "<br>\n",
    "\n",
    "**Extracting Olist Datasets with a Python script**<br>The [extract_datasets.py](https://github.com/YvonneLipLim/JDE05_Final_Project/blob/main/Data_Ingestion/extract_datasets.py) file will download the datasets into the local directory which can be uploaded to the Azure storage account using Azure CLI method.<br><br>\n",
    "\n",
    "How to Upload Olist Datasets with Azure CLI<br>\n",
    "1. Run the following command in a new Terminal:\n",
    "    ```sh\n",
    "    az login\n",
    "    ```   \n",
    "2. A web page will be opened, asking you to enter your Azure credentials. Log in with your account.\n",
    "3. After logging in, you should see a message in the terminal indicating that you have successfully logged in, along with a list of your subscriptions.\n",
    "4. Once logged in, run the following command:\n",
    "    ```sh\n",
    "    az storage blob upload-batch -d <Container-Name>/<Directory-Name> --account-name <Storage-Account-Name> -s <local-directory-path>\n",
    "    ```\n",
    "5. Verify the upload by logging in to the Azure portal to confirm that all your CSV files have been uploaded to the specified container.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/YvonneLipLim/Images/main/Upload_Datasets_Azure.png\" alt=\"Upload Datasets with Azure CLI\" style=\"display: block; margin: 0 auto\" /><br><br>   \n",
    "\n",
    "**Method 2**<br>Azure Function App\n",
    "- Rewrite the Python Azure Function script by editing the [function_app.py](https://github.com/YvonneLipLim/JDE05_Final_Project/blob/main/Data_Ingestion/function_app.py) file which can be found in your local directory. The script includes:\n",
    "  - A scheduled function `scheduled_kaggle_data_extractor` that runs daily at midnight.\n",
    "  - A manually triggered function `manual_kaggle_data_extractor` that can be invoked via an HTTP request.\n",
    "  - A common `kaggle_data_extractor` function that both the scheduled and manual functions call to perform the data extraction and upload.\n",
    "- Then open a new Terminal and start the Azure Storage Emulator:\n",
    "  ```sh\n",
    "  azurite --silent --location\n",
    "  ```\n",
    "- Next, run the following command in a new Terminal to trigger the scheduled function\n",
    "  ```sh\n",
    "  cd MyFunctionApp\n",
    "  func new --template \"Timer trigger\" --name KaggleDataExtractor\n",
    "  0 0 0 * * *\n",
    "  func start --verbose\n",
    "  ```\n",
    "- To run the manual trigger, you can run the curl command:\n",
    "  ```sh\n",
    "  curl http://localhost:7071/api/manually-trigger-kaggle-extractor\n",
    "  ```\n",
    "- Verify the upload by checking if the files exists in the specified directory of the storage container in Azure.<br><br>\n",
    "\n",
    "**Example of a Manual Trigger Upload**<br>\n",
    "<img src=\"https://raw.githubusercontent.com/YvonneLipLim/Images/main/Storage_Container_Files.png\" alt=\"Storage Container Files Manual Trigger\" style=\"display: block; margin: 0 auto\" /><br> \n",
    "\n",
    "**Example of a Scheduled Trigger Upload**<br>\n",
    "<img src=\"https://raw.githubusercontent.com/YvonneLipLim/Images/main/Storage_Container_Files_Automate.png\" alt=\"Storage Container Files Automate\" style=\"display: block; margin: 0 auto\" /><br>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/YvonneLipLim/Images/main/Data_Ingestion.png\" alt=\"Data Ingestion\" style=\"display: block; margin: 0 auto\" /><br><br> \n",
    "\n",
    "#### **Mounting Data from Azure Data Lake Gen 2 to Databricks**<br>\n",
    "\n",
    "</details>\n",
    "<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n",
    "<br><br>\n",
    "\n",
    "<!-- DATA TRANSFORMATION -->\n",
    "<details>\n",
    "  <summary><h3><b>Data Transformation</b></h3></summary>\n",
    "\n",
    "</details>\n",
    "<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n",
    "<br><br>\n",
    "\n",
    "<!-- DATA WAREHOUSING -->\n",
    "<details>\n",
    "  <summary><h3><b>Data Warehousing</b></h3></summary>\n",
    "\n",
    "</details>\n",
    "<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n",
    "<br><br>\n",
    "\n",
    "<!-- DASHBOARDING -->\n",
    "<details>\n",
    "  <summary><h3><b>Dashboarding</b></h3></summary>\n",
    "\n",
    "The magic of Power BI brings the data to life through captivating and enlightening revenue prediction dashboards, empowering the e-commerce business with invaluable data-driven insights for astute and informed strategic decision-making.<br>\n",
    "These interactive dashboards provide a visual representation of the predicted revenue trends, enabling stakeholders to monitor performance and identify growth opportunities easily.\n",
    "\n",
    "</details>\n",
    "<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n",
    "<br><br>\n",
    "\n",
    "<!-- DATA GOVERNANCE AND SECURITY -->\n",
    "<details>\n",
    "  <summary><h3><b>Data Governance and Security</b></h3></summary>\n",
    "\n",
    "##### **Database security**\n",
    "Database security is a critical aspect of managing any database, and Olist stores, being an e-commerce platform dealing with sensitive customer and business data, requires robust security measures. The following are steps/methods to ensure database security for the olist store database:\n",
    "1. Access Control and Authentication: using role-based access control (RBAC) to grant specific privileges to different user roles based on their responsibilities.\n",
    "2. Database Auditing: using triggers to provide an audit trail to track and log all user activities and security-relevant events.\n",
    "3. Backup and Recovery:backing up the database — ensuring they are securely stored — testing the restoration process to verify that backups are reliable and can be used for recovery in case of data loss or security incidents.\n",
    "\n",
    "</details>\n",
    "<p align=\"right\">(<a href=\"#readme-top\">back to top</a>)</p>\n",
    "<br><br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
